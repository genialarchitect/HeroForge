import{j as e}from"./vendor-state-DdX3IZQk.js";import{ah as s,z as r,J as i,aD as l,p as n,a as d}from"./vendor-ui-DU7fCopR.js";import"./vendor-react-Da1CiaFS.js";const m=()=>{const a=[{name:"Prompt Injection",icon:e.jsx(r,{className:"h-5 w-5"}),color:"text-red-500",description:"Test for malicious prompt injection attacks",testCount:25},{name:"Jailbreak Attempts",icon:e.jsx(i,{className:"h-5 w-5"}),color:"text-orange-500",description:"Bypass safety guardrails and content filters",testCount:18},{name:"Data Extraction",icon:e.jsx(l,{className:"h-5 w-5"}),color:"text-yellow-500",description:"Attempt to extract training data or sensitive information",testCount:12},{name:"Context Manipulation",icon:e.jsx(n,{className:"h-5 w-5"}),color:"text-purple-500",description:"Manipulate conversation context for unintended behavior",testCount:9},{name:"Encoding Attacks",icon:e.jsx(d,{className:"h-5 w-5"}),color:"text-blue-500",description:"Use encoding techniques to bypass filters",testCount:7}];return e.jsxs("div",{className:"space-y-6",children:[e.jsx("div",{className:"flex items-center justify-between",children:e.jsxs("div",{children:[e.jsxs("h1",{className:"text-3xl font-bold text-slate-900 dark:text-white flex items-center gap-3",children:[e.jsx(s,{className:"h-8 w-8 text-primary"}),"LLM Security Testing"]}),e.jsx("p",{className:"text-slate-600 dark:text-slate-400 mt-2",children:"Test AI and LLM applications for security vulnerabilities"})]})}),e.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6",children:[e.jsxs("div",{className:"bg-gradient-to-br from-purple-500 to-purple-600 rounded-lg p-6 text-white",children:[e.jsx("div",{className:"text-3xl font-bold mb-2",children:"69"}),e.jsx("div",{className:"text-purple-100",children:"Built-in Test Cases"})]}),e.jsxs("div",{className:"bg-gradient-to-br from-blue-500 to-blue-600 rounded-lg p-6 text-white",children:[e.jsx("div",{className:"text-3xl font-bold mb-2",children:"5"}),e.jsx("div",{className:"text-blue-100",children:"Attack Categories"})]}),e.jsxs("div",{className:"bg-gradient-to-br from-green-500 to-green-600 rounded-lg p-6 text-white",children:[e.jsx("div",{className:"text-3xl font-bold mb-2",children:"0"}),e.jsx("div",{className:"text-green-100",children:"Tests Run"})]})]}),e.jsxs("div",{children:[e.jsx("h2",{className:"text-xl font-semibold text-slate-900 dark:text-white mb-4",children:"Test Categories"}),e.jsx("div",{className:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4",children:a.map(t=>e.jsxs("div",{className:"bg-light-surface dark:bg-dark-surface border border-light-border dark:border-dark-border rounded-lg p-5 hover:shadow-lg transition-shadow",children:[e.jsxs("div",{className:"flex items-start justify-between mb-3",children:[e.jsx("div",{className:`${t.color}`,children:t.icon}),e.jsxs("span",{className:"text-xs font-medium px-2 py-1 bg-slate-100 dark:bg-slate-800 text-slate-700 dark:text-slate-300 rounded",children:[t.testCount," tests"]})]}),e.jsx("h3",{className:"font-semibold text-slate-900 dark:text-white mb-2",children:t.name}),e.jsx("p",{className:"text-sm text-slate-600 dark:text-slate-400",children:t.description})]},t.name))})]}),e.jsx("div",{className:"bg-purple-50 dark:bg-purple-900/20 border border-purple-200 dark:border-purple-800 rounded-lg p-6",children:e.jsxs("div",{className:"flex items-start gap-3",children:[e.jsx(s,{className:"h-5 w-5 text-purple-600 dark:text-purple-400 mt-0.5"}),e.jsxs("div",{children:[e.jsx("h4",{className:"font-semibold text-purple-900 dark:text-purple-100 mb-2",children:"Comprehensive LLM Security Testing"}),e.jsx("p",{className:"text-sm text-purple-800 dark:text-purple-200",children:"Test your AI applications against 69 built-in test cases covering prompt injection, jailbreak attempts, data extraction, context manipulation, and encoding attacks. Identify vulnerabilities before attackers do."})]})]})})]})};export{m as default};
